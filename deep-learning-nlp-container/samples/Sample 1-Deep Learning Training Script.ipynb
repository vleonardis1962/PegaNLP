{
  "cells": [
    {
      "source": [
        "!pip install nltk"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "XJ1ImtYACDGX",
        "outputId": "442d4958-ada1-47b3-8f0d-a0d9f2e19abf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iqjFblSIBrgT",
        "outputId": "80c2ad27-5be6-4a8f-ecef-810258a50a5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "2.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#Generic Packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import os\n",
        "import re\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "#Keras Packages\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras import utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,Activation\n",
        "from keras import metrics\n",
        "from sklearn import preprocessing\n",
        "from keras import backend\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "\n",
        "import keras\n",
        "print(keras.__version__)\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#NLTK Packages\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "#Spliting Package\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from keras.models import model_from_json\n",
        "from keras.models import model_from_yaml\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YhcXePewBrgV"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/sample_data/california_housing_test.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (data)"
      ],
      "metadata": {
        "id": "UVBR2G4pGG-Q",
        "outputId": "803fea6e-979e-4e03-d23b-7e688ae30b1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
            "0       -122.05     37.37                27.0       3885.0           661.0   \n",
            "1       -118.30     34.26                43.0       1510.0           310.0   \n",
            "2       -117.81     33.78                27.0       3589.0           507.0   \n",
            "3       -118.36     33.82                28.0         67.0            15.0   \n",
            "4       -119.67     36.33                19.0       1241.0           244.0   \n",
            "...         ...       ...                 ...          ...             ...   \n",
            "2995    -119.86     34.42                23.0       1450.0           642.0   \n",
            "2996    -118.14     34.06                27.0       5257.0          1082.0   \n",
            "2997    -119.70     36.30                10.0        956.0           201.0   \n",
            "2998    -117.12     34.10                40.0         96.0            14.0   \n",
            "2999    -119.63     34.42                42.0       1765.0           263.0   \n",
            "\n",
            "      population  households  median_income  median_house_value  \n",
            "0         1537.0       606.0         6.6085            344700.0  \n",
            "1          809.0       277.0         3.5990            176500.0  \n",
            "2         1484.0       495.0         5.7934            270500.0  \n",
            "3           49.0        11.0         6.1359            330000.0  \n",
            "4          850.0       237.0         2.9375             81700.0  \n",
            "...          ...         ...            ...                 ...  \n",
            "2995      1258.0       607.0         1.1790            225000.0  \n",
            "2996      3496.0      1036.0         3.3906            237200.0  \n",
            "2997       693.0       220.0         2.2895             62000.0  \n",
            "2998        46.0        14.0         3.2708            162500.0  \n",
            "2999       753.0       260.0         8.5608            500001.0  \n",
            "\n",
            "[3000 rows x 9 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XbXNfQ3eBrgV"
      },
      "outputs": [],
      "source": [
        "#Regex for any data cleansing activities\n",
        "def RegexRemoval(data):\n",
        "    removed_spc=[]\n",
        "    for i in data['Content']:\n",
        "        removal_spc=re.sub('[^a-zA-Z]',' ',i)\n",
        "        removal_spc=re.sub(r'\\.+','.', removal_spc)\n",
        "        removed_spc.append(removal_spc)\n",
        "    return removed_spc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjRDnmD9BrgW"
      },
      "outputs": [],
      "source": [
        "#feature engineering steps like stemming,lemmatization,tokenization can be handled below\n",
        "def cleanData(text, lowercase = True, remove_stops = True, stemming = False, lemmatization = False):\n",
        "\n",
        "    txt = str(text)\n",
        "    if lowercase:\n",
        "        txt = \" \".join([w.lower() for w in txt.split()])\n",
        "\n",
        "    if remove_stops:\n",
        "        txt = \" \".join([w for w in txt.split() if w not in stop])\n",
        "\n",
        "    if stemming:\n",
        "        st = PorterStemmer() #choose different stemmers like lancaster for testing activities\n",
        "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
        "\n",
        "    if lemmatization:\n",
        "        wordnet_lemmatizer = WordNetLemmatizer()\n",
        "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n",
        "    return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zFWqn-eBrgW"
      },
      "outputs": [],
      "source": [
        "def MLSplit(data):\n",
        "    X = data.content\n",
        "    y = data.result\n",
        "    print(type(X))\n",
        "    print(type(y))\n",
        "    size = 0.1\n",
        "\n",
        "    #Stratified shuffle Split is used. Please use random split(if required)\n",
        "    dataSplit = StratifiedShuffleSplit(n_splits=5, test_size=size, random_state=0)\n",
        "    for train_index, validation_index in dataSplit.split(X,y):\n",
        "        X_train, X_validation = X[train_index], X[validation_index]\n",
        "        y_train, y_validation = y[train_index], y[validation_index]\n",
        "\n",
        "    X_train = X_train[:]\n",
        "    X_validation = X_validation[:]\n",
        "    print(type(X_train))\n",
        "    print(type(y_train))\n",
        "\n",
        "    trainData = pd.concat([X_train,y_train],axis=1)\n",
        "    validateData = pd.concat([X_validation,y_validation],axis=1)\n",
        "    print(\"Train Data Features:\",trainData.shape)\n",
        "    print(\"Validation Data Features:\",validateData.shape)\n",
        "\n",
        "    return X_train,y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gY-VW2rJBrgW"
      },
      "outputs": [],
      "source": [
        "def DLTrain(data):\n",
        "\n",
        "    #RegEx to remove the alpha numerical data\n",
        "    cleanedData=RegexRemoval(data)\n",
        "    cleanedData = pd.DataFrame(cleanedData)\n",
        "    cleanedData.rename(columns={0:'content'},inplace=True)\n",
        "\n",
        "    data = pd.concat([data,cleanedData],axis=1)\n",
        "\n",
        "    #Pre-Procesed Data Frame\n",
        "    data = data[['content','Result']]\n",
        "    data.rename(columns={'Result':'result'},inplace=True)\n",
        "    print(data.head())\n",
        "\n",
        "    data['content'] = data['content'].map(lambda x: cleanData(x, lowercase=False, remove_stops=True, stemming=False, lemmatization = False))\n",
        "\n",
        "    X,y = MLSplit(data)\n",
        "   ########################### Hyper Parameter Configurations #####################\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=1000, activation='relu', input_shape=(1000,)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=1024, activation='relu', input_shape=(1000,)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=6, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc',metrics.categorical_accuracy])\n",
        "    #################################################################################\n",
        "\n",
        "    #Keras Tokenizer\n",
        "    num_max = 1000\n",
        "    tok = Tokenizer(num_words=num_max)\n",
        "    tok.fit_on_texts(X)\n",
        "    X = tok.texts_to_matrix(X,mode='count')\n",
        "\n",
        "    #Label Encoder\n",
        "    encoder=preprocessing.LabelEncoder()\n",
        "    encoder.fit(y)\n",
        "    y=encoder.transform(y)\n",
        "    num_classes = np.max(y) + 1\n",
        "    y = utils.to_categorical(y,num_classes)\n",
        "\n",
        "    #Model Building\n",
        "    model.fit(X, y, epochs=10, batch_size=500,verbose=1,validation_split=0.2)\n",
        "\n",
        "    model_yaml = model.to_yaml()\n",
        "    model_yaml\n",
        "\n",
        "    model_json = model.to_json()\n",
        "    model_json\n",
        "\n",
        "    #saving The Models\n",
        "    with open('..\\\\models\\\\tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(tok,f)\n",
        "\n",
        "    with open(\"..\\\\models\\\\model.yaml\", \"w\") as yaml_file:\n",
        "        yaml_file.write(model_yaml)\n",
        "\n",
        "    with open(\"..\\\\models\\\\model.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "\n",
        "    model.save_weights(\"..\\\\models\\\\model.h5\")\n",
        "    print(\"Saved model to disk\")\n",
        "\n",
        "    with open('..\\\\models\\\\encoder.pkl', 'wb') as f:\n",
        "        pickle.dump(encoder,f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj8ZtgLdBrgX",
        "outputId": "663de0e3-4b9e-4784-f329-8ab9c803ec59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                     content             result\n",
            "0      good morning everyone  action > greeting\n",
            "1       have a great morning  action > greeting\n",
            "2  and a good morning to you  action > greeting\n",
            "3        good morning to you  action > greeting\n",
            "4         hello good morning  action > greeting\n",
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "Train Data Features: (1115, 2)\n",
            "Validation Data Features: (124, 2)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1000)              1001000   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              1025024   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 6)                 6150      \n",
            "=================================================================\n",
            "Total params: 2,032,174\n",
            "Trainable params: 2,032,174\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 892 samples, validate on 223 samples\n",
            "Epoch 1/10\n",
            "892/892 [==============================] - 1s 1ms/step - loss: 1.6885 - acc: 0.3352 - categorical_accuracy: 0.3352 - val_loss: 1.4091 - val_acc: 0.4843 - val_categorical_accuracy: 0.4843\n",
            "Epoch 2/10\n",
            "892/892 [==============================] - 1s 601us/step - loss: 1.3616 - acc: 0.4608 - categorical_accuracy: 0.4608 - val_loss: 1.2040 - val_acc: 0.4843 - val_categorical_accuracy: 0.4843\n",
            "Epoch 3/10\n",
            "892/892 [==============================] - 1s 620us/step - loss: 1.1327 - acc: 0.4608 - categorical_accuracy: 0.4608 - val_loss: 1.0135 - val_acc: 0.4843 - val_categorical_accuracy: 0.4843\n",
            "Epoch 4/10\n",
            "892/892 [==============================] - 1s 627us/step - loss: 0.9499 - acc: 0.4742 - categorical_accuracy: 0.4742 - val_loss: 0.9251 - val_acc: 0.5740 - val_categorical_accuracy: 0.5740\n",
            "Epoch 5/10\n",
            "892/892 [==============================] - 1s 627us/step - loss: 0.8219 - acc: 0.6413 - categorical_accuracy: 0.6413 - val_loss: 0.7457 - val_acc: 0.7444 - val_categorical_accuracy: 0.7444\n",
            "Epoch 6/10\n",
            "892/892 [==============================] - 1s 622us/step - loss: 0.6050 - acc: 0.8274 - categorical_accuracy: 0.8274 - val_loss: 0.5939 - val_acc: 0.8520 - val_categorical_accuracy: 0.8520\n",
            "Epoch 7/10\n",
            "892/892 [==============================] - 1s 747us/step - loss: 0.4519 - acc: 0.8845 - categorical_accuracy: 0.8845 - val_loss: 0.7626 - val_acc: 0.7309 - val_categorical_accuracy: 0.7309\n",
            "Epoch 8/10\n",
            "892/892 [==============================] - 1s 644us/step - loss: 0.5363 - acc: 0.7915 - categorical_accuracy: 0.7915 - val_loss: 0.4941 - val_acc: 0.8565 - val_categorical_accuracy: 0.8565\n",
            "Epoch 9/10\n",
            "892/892 [==============================] - 1s 584us/step - loss: 0.3071 - acc: 0.9316 - categorical_accuracy: 0.9316 - val_loss: 0.4311 - val_acc: 0.8700 - val_categorical_accuracy: 0.8700\n",
            "Epoch 10/10\n",
            "892/892 [==============================] - 1s 598us/step - loss: 0.2547 - acc: 0.9316 - categorical_accuracy: 0.9316 - val_loss: 0.4261 - val_acc: 0.8655 - val_categorical_accuracy: 0.8655\n",
            "Saved model to disk\n"
          ]
        }
      ],
      "source": [
        "DLTrain(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz5F7ZkmBrgX"
      },
      "outputs": [],
      "source": [
        "#The below is just for validation purpose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpBJS2UABrgX"
      },
      "outputs": [],
      "source": [
        "def validation(text):\n",
        "\n",
        "    #Loading Tokenizer\n",
        "    with open('..\\\\models\\\\tokenizer.pkl', 'rb') as f:\n",
        "        tok = pickle.load(f)\n",
        "\n",
        "    #loading hyperparameter\n",
        "    with open(\"..\\\\models\\\\model.yaml\", \"r\") as yaml_file:\n",
        "        model_yaml = yaml_file.read()\n",
        "\n",
        "    #loading model\n",
        "    model = model_from_yaml(model_yaml)\n",
        "    model.load_weights(\"..\\\\models\\\\model.h5\")\n",
        "\n",
        "    #loading encoder for converting encoded labels to actual labels\n",
        "    with open('..\\\\models\\\\encoder.pkl', 'rb') as f:\n",
        "        encoder = pickle.load(f)\n",
        "\n",
        "    #inline text\n",
        "    testdataL = text\n",
        "\n",
        "\n",
        "\n",
        "    #tokenization of inline text\n",
        "    X_test=tok.texts_to_matrix(testdataL,mode='count')\n",
        "\n",
        "    #predicting for inline text\n",
        "    prediction = model.predict(np.array(X_test))\n",
        "\n",
        "    #exracting the labels for the predicted text\n",
        "    text_labels = encoder.classes_\n",
        "    predicted_label = text_labels[np.argmax(prediction)]\n",
        "    print(\"predicted category -->\",predicted_label)\n",
        "\n",
        "    prediction_prob = model.predict_proba(np.array([X_test[0]]))\n",
        "    confidence = prediction_prob[0][np.argmax(prediction_prob)]\n",
        "    print(\"confidence score -->\",str(round(confidence,2)))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2suAqC8_BrgY",
        "outputId": "e1d5d03d-d0d3-4684-8ad2-d61877b38ef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted category --> action > escalate\n",
            "confidence score --> 0.74\n"
          ]
        }
      ],
      "source": [
        "text = [\"I want to talk to your manager\"]\n",
        "validation(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CATwYvuoBrgY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}