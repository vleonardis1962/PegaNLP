{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8l5rDBJ2I6hS",
        "outputId": "7fee2c22-9679-4e40-8e29-d252bf3bcc44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "2.15.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-245b9eb58051>\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#NLTK Packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#Spliting Package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ],
      "source": [
        "#Generic Packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import os\n",
        "import re\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "#Keras Packages\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras import utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,Activation\n",
        "from keras import metrics\n",
        "from sklearn import preprocessing\n",
        "from keras import backend\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "\n",
        "import keras\n",
        "print(keras.__version__)\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#NLTK Packages\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "#Spliting Package\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from keras.models import model_from_json\n",
        "from keras.models import model_from_yaml\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwRw5z3gI6hV"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"..\\\\kaggle_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGJuOHAiI6hW"
      },
      "outputs": [],
      "source": [
        "#Regex for any data cleansing activities\n",
        "def RegexRemoval(data):\n",
        "    removed_spc=[]\n",
        "    for i in data['Content']:\n",
        "        removal_spc=re.sub('[^a-zA-Z]',' ',i)\n",
        "        removal_spc=re.sub(r'\\.+','.', removal_spc)\n",
        "        removed_spc.append(removal_spc)\n",
        "    return removed_spc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zH-Ke52_I6hW"
      },
      "outputs": [],
      "source": [
        "#feature engineering steps like stemming,lemmatization,tokenization can be handled below\n",
        "def cleanData(text, lowercase = True, remove_stops = True, stemming = False, lemmatization = False):\n",
        "\n",
        "    txt = str(text)\n",
        "    if lowercase:\n",
        "        txt = \" \".join([w.lower() for w in txt.split()])\n",
        "\n",
        "    if remove_stops:\n",
        "        txt = \" \".join([w for w in txt.split() if w not in stop])\n",
        "\n",
        "    if stemming:\n",
        "        st = PorterStemmer() #choose different stemmers like lancaster for testing activities\n",
        "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
        "\n",
        "    if lemmatization:\n",
        "        wordnet_lemmatizer = WordNetLemmatizer()\n",
        "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n",
        "    return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rywAfMsyI6hX"
      },
      "outputs": [],
      "source": [
        "def MLSplit(data):\n",
        "    X = data.content\n",
        "    y = data.result\n",
        "    print(type(X))\n",
        "    print(type(y))\n",
        "    size = 0.1\n",
        "\n",
        "    #Stratified shuffle Split is used. Please use random split(if required)\n",
        "    dataSplit = StratifiedShuffleSplit(n_splits=5, test_size=size, random_state=0)\n",
        "    for train_index, validation_index in dataSplit.split(X,y):\n",
        "        X_train, X_validation = X[train_index], X[validation_index]\n",
        "        y_train, y_validation = y[train_index], y[validation_index]\n",
        "\n",
        "    X_train = X_train[:]\n",
        "    X_validation = X_validation[:]\n",
        "    print(type(X_train))\n",
        "    print(type(y_train))\n",
        "\n",
        "    trainData = pd.concat([X_train,y_train],axis=1)\n",
        "    validateData = pd.concat([X_validation,y_validation],axis=1)\n",
        "    print(\"Train Data Features:\",trainData.shape)\n",
        "    print(\"Validation Data Features:\",validateData.shape)\n",
        "\n",
        "    return X_train,y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpNSZWlrI6hX"
      },
      "outputs": [],
      "source": [
        "def DLTrain(data):\n",
        "\n",
        "    #RegEx to remove the alpha numerical data\n",
        "    cleanedData=RegexRemoval(data)\n",
        "    cleanedData = pd.DataFrame(cleanedData)\n",
        "    cleanedData.rename(columns={0:'content'},inplace=True)\n",
        "\n",
        "    data = pd.concat([data,cleanedData],axis=1)\n",
        "\n",
        "    #Pre-Procesed Data Frame\n",
        "    data = data[['content','Result']]\n",
        "    data.rename(columns={'Result':'result'},inplace=True)\n",
        "    print(data.head())\n",
        "\n",
        "    data['content'] = data['content'].map(lambda x: cleanData(x, lowercase=False, remove_stops=True, stemming=False, lemmatization = False))\n",
        "\n",
        "    X,y = MLSplit(data)\n",
        "   ########################### Hyper Parameter Configurations #####################\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=1000, activation='relu', input_shape=(1000,)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=1024, activation='relu', input_shape=(1000,)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=9, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc',metrics.categorical_accuracy])\n",
        "    #################################################################################\n",
        "\n",
        "    #Keras Tokenizer\n",
        "    num_max = 1000\n",
        "    tok = Tokenizer(num_words=num_max)\n",
        "    tok.fit_on_texts(X)\n",
        "    X = tok.texts_to_matrix(X,mode='count')\n",
        "\n",
        "    #Label Encoder\n",
        "    encoder=preprocessing.LabelEncoder()\n",
        "    encoder.fit(y)\n",
        "    y=encoder.transform(y)\n",
        "    num_classes = np.max(y) + 1\n",
        "    y = utils.to_categorical(y,num_classes)\n",
        "\n",
        "    #Model Building\n",
        "    model.fit(X, y, epochs=10, batch_size=500,verbose=1,validation_split=0.2)\n",
        "\n",
        "    model_yaml = model.to_yaml()\n",
        "    model_yaml\n",
        "\n",
        "    model_json = model.to_json()\n",
        "    model_json\n",
        "\n",
        "    #saving The Models\n",
        "    with open('..\\\\models\\\\tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(tok,f)\n",
        "\n",
        "    with open(\"..\\\\models\\\\model.yaml\", \"w\") as yaml_file:\n",
        "        yaml_file.write(model_yaml)\n",
        "\n",
        "    with open(\"..\\\\models\\\\model.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "\n",
        "    model.save_weights(\"..\\\\models\\\\model.h5\")\n",
        "    print(\"Saved model to disk\")\n",
        "\n",
        "    with open('..\\\\models\\\\encoder.pkl', 'wb') as f:\n",
        "        pickle.dump(encoder,f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BVp5lm-I6hY",
        "outputId": "f3337952-af22-4439-f776-75a8f96595c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                             content  \\\n",
            "0  I was contacted originally on XX XX XXXX via c...   \n",
            "1  Thanks for your response and update regarding ...   \n",
            "2  Dear Consumers Financial Protection Bureau   C...   \n",
            "3  Experian XXXX XXXX XXXX XXXX XXXX XXXX XXXX  X...   \n",
            "4  Doctors Business Bureau for            That de...   \n",
            "\n",
            "                      result  \n",
            "0   Action > Debt collection  \n",
            "1          Action > Mortgage  \n",
            "2      Action > Student loan  \n",
            "3  Action > Credit reporting  \n",
            "4   Action > Debt collection  \n",
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "Train Data Features: (6823, 2)\n",
            "Validation Data Features: (759, 2)\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 1000)              1001000   \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1024)              1025024   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 9)                 9225      \n",
            "=================================================================\n",
            "Total params: 2,035,249\n",
            "Trainable params: 2,035,249\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 5458 samples, validate on 1365 samples\n",
            "Epoch 1/10\n",
            "5458/5458 [==============================] - 4s 662us/step - loss: 1.7663 - acc: 0.3193 - categorical_accuracy: 0.3193 - val_loss: 1.4339 - val_acc: 0.3993 - val_categorical_accuracy: 0.3993\n",
            "Epoch 2/10\n",
            "5458/5458 [==============================] - 3s 581us/step - loss: 1.1646 - acc: 0.4152 - categorical_accuracy: 0.4152 - val_loss: 1.1113 - val_acc: 0.5392 - val_categorical_accuracy: 0.5392\n",
            "Epoch 3/10\n",
            "5458/5458 [==============================] - 3s 574us/step - loss: 0.8818 - acc: 0.6755 - categorical_accuracy: 0.6755 - val_loss: 0.7259 - val_acc: 0.7905 - val_categorical_accuracy: 0.7905\n",
            "Epoch 4/10\n",
            "5458/5458 [==============================] - 3s 587us/step - loss: 0.5889 - acc: 0.8074 - categorical_accuracy: 0.8074 - val_loss: 0.7006 - val_acc: 0.7985 - val_categorical_accuracy: 0.7985\n",
            "Epoch 5/10\n",
            "5458/5458 [==============================] - 3s 614us/step - loss: 0.4462 - acc: 0.8540 - categorical_accuracy: 0.8540 - val_loss: 0.6083 - val_acc: 0.8278 - val_categorical_accuracy: 0.8278\n",
            "Epoch 6/10\n",
            "5458/5458 [==============================] - 3s 574us/step - loss: 0.3162 - acc: 0.8943 - categorical_accuracy: 0.8943 - val_loss: 1.0469 - val_acc: 0.7297 - val_categorical_accuracy: 0.7297\n",
            "Epoch 7/10\n",
            "5458/5458 [==============================] - 3s 572us/step - loss: 0.2926 - acc: 0.8992 - categorical_accuracy: 0.8992 - val_loss: 0.7973 - val_acc: 0.8059 - val_categorical_accuracy: 0.8059\n",
            "Epoch 8/10\n",
            "5458/5458 [==============================] - 3s 573us/step - loss: 0.2522 - acc: 0.9159 - categorical_accuracy: 0.9159 - val_loss: 0.6564 - val_acc: 0.8264 - val_categorical_accuracy: 0.8264\n",
            "Epoch 9/10\n",
            "5458/5458 [==============================] - 3s 570us/step - loss: 0.1668 - acc: 0.9460 - categorical_accuracy: 0.9460 - val_loss: 0.7052 - val_acc: 0.8344 - val_categorical_accuracy: 0.8344\n",
            "Epoch 10/10\n",
            "5458/5458 [==============================] - 3s 557us/step - loss: 0.1135 - acc: 0.9654 - categorical_accuracy: 0.9654 - val_loss: 0.9879 - val_acc: 0.7832 - val_categorical_accuracy: 0.7832\n",
            "Saved model to disk\n"
          ]
        }
      ],
      "source": [
        "DLTrain(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TmM_4DJI6hY"
      },
      "outputs": [],
      "source": [
        "def validation(text):\n",
        "\n",
        "    #Loading Tokenizer\n",
        "    with open('..\\\\models\\\\tokenizer.pkl', 'rb') as f:\n",
        "        tok = pickle.load(f)\n",
        "\n",
        "    #loading hyperparameter\n",
        "    with open(\"..\\\\models\\\\model.yaml\", \"r\") as yaml_file:\n",
        "        model_yaml = yaml_file.read()\n",
        "\n",
        "    #loading model\n",
        "    model = model_from_yaml(model_yaml)\n",
        "    model.load_weights(\"..\\\\models\\\\model.h5\")\n",
        "\n",
        "    #loading encoder for converting encoded labels to actual labels\n",
        "    with open('..\\\\models\\\\encoder.pkl', 'rb') as f:\n",
        "        encoder = pickle.load(f)\n",
        "\n",
        "    #inline text\n",
        "    testdataL = text\n",
        "\n",
        "\n",
        "\n",
        "    #tokenization of inline text\n",
        "    X_test=tok.texts_to_matrix(testdataL,mode='count')\n",
        "\n",
        "    #predicting for inline text\n",
        "    prediction = model.predict(np.array(X_test))\n",
        "\n",
        "    #exracting the labels for the predicted text\n",
        "    text_labels = encoder.classes_\n",
        "    predicted_label = text_labels[np.argmax(prediction)]\n",
        "    print(\"predicted category -->\",predicted_label)\n",
        "\n",
        "    prediction_prob = model.predict_proba(np.array([X_test[0]]))\n",
        "    confidence = prediction_prob[0][np.argmax(prediction_prob)]\n",
        "    print(\"confidence score -->\",str(round(confidence,2)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUeEed0sI6hY",
        "outputId": "22e8f061-3209-4566-89ba-c907ed1f0676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted category --> Action > Student loan\n",
            "confidence score --> 0.56\n"
          ]
        }
      ],
      "source": [
        "text = [\"I want a student loan\"]\n",
        "validation(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4823CBKCI6hZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}