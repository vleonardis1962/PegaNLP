{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8l5rDBJ2I6hS",
        "outputId": "dfdc667b-95e8-4b47-b744-301405f7328e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n",
            "2.15.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "#Generic Packages\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import os\n",
        "import re\n",
        "import joblib\n",
        "import pickle\n",
        "\n",
        "#Keras Packages\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras import utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,Activation\n",
        "from keras import metrics\n",
        "from sklearn import preprocessing\n",
        "from keras import backend\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "\n",
        "import keras\n",
        "print(keras.__version__)\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#NLTK Packages\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "#Spliting Package\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "from keras.models import model_from_json\n",
        "from keras.models import model_from_yaml\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "uwRw5z3gI6hV"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/kaggle_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (data)"
      ],
      "metadata": {
        "id": "IMz5JucsLfgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qGJuOHAiI6hW"
      },
      "outputs": [],
      "source": [
        "#Regex for any data cleansing activities\n",
        "def RegexRemoval(data):\n",
        "    removed_spc=[]\n",
        "    for i in data['Content']:\n",
        "        removal_spc=re.sub('[^a-zA-Z]',' ',i)\n",
        "        removal_spc=re.sub(r'\\.+','.', removal_spc)\n",
        "        removed_spc.append(removal_spc)\n",
        "    return removed_spc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zH-Ke52_I6hW"
      },
      "outputs": [],
      "source": [
        "#feature engineering steps like stemming,lemmatization,tokenization can be handled below\n",
        "def cleanData(text, lowercase = True, remove_stops = True, stemming = False, lemmatization = False):\n",
        "\n",
        "    txt = str(text)\n",
        "    if lowercase:\n",
        "        txt = \" \".join([w.lower() for w in txt.split()])\n",
        "\n",
        "    if remove_stops:\n",
        "        txt = \" \".join([w for w in txt.split() if w not in stop])\n",
        "\n",
        "    if stemming:\n",
        "        st = PorterStemmer() #choose different stemmers like lancaster for testing activities\n",
        "        txt = \" \".join([st.stem(w) for w in txt.split()])\n",
        "\n",
        "    if lemmatization:\n",
        "        wordnet_lemmatizer = WordNetLemmatizer()\n",
        "        txt = \" \".join([wordnet_lemmatizer.lemmatize(w, pos='v') for w in txt.split()])\n",
        "    return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rywAfMsyI6hX"
      },
      "outputs": [],
      "source": [
        "def MLSplit(data):\n",
        "    X = data.content\n",
        "    y = data.result\n",
        "    print(type(X))\n",
        "    print(type(y))\n",
        "    size = 0.1\n",
        "\n",
        "    #Stratified shuffle Split is used. Please use random split(if required)\n",
        "    dataSplit = StratifiedShuffleSplit(n_splits=5, test_size=size, random_state=0)\n",
        "    for train_index, validation_index in dataSplit.split(X,y):\n",
        "        X_train, X_validation = X[train_index], X[validation_index]\n",
        "        y_train, y_validation = y[train_index], y[validation_index]\n",
        "\n",
        "    X_train = X_train[:]\n",
        "    X_validation = X_validation[:]\n",
        "    print(type(X_train))\n",
        "    print(type(y_train))\n",
        "\n",
        "    trainData = pd.concat([X_train,y_train],axis=1)\n",
        "    validateData = pd.concat([X_validation,y_validation],axis=1)\n",
        "    print(\"Train Data Features:\",trainData.shape)\n",
        "    print(\"Validation Data Features:\",validateData.shape)\n",
        "\n",
        "    return X_train,y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "IpNSZWlrI6hX"
      },
      "outputs": [],
      "source": [
        "def DLTrain(data):\n",
        "\n",
        "    #RegEx to remove the alpha numerical data\n",
        "    cleanedData=RegexRemoval(data)\n",
        "    cleanedData = pd.DataFrame(cleanedData)\n",
        "    cleanedData.rename(columns={0:'content'},inplace=True)\n",
        "\n",
        "    data = pd.concat([data,cleanedData],axis=1)\n",
        "\n",
        "    #Pre-Procesed Data Frame\n",
        "    data = data[['content','Result']]\n",
        "    data.rename(columns={'Result':'result'},inplace=True)\n",
        "    print(data.head())\n",
        "\n",
        "    data['content'] = data['content'].map(lambda x: cleanData(x, lowercase=False, remove_stops=True, stemming=False, lemmatization = False))\n",
        "\n",
        "    X,y = MLSplit(data)\n",
        "   ########################### Hyper Parameter Configurations #####################\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=1000, activation='relu', input_shape=(1000,)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=1024, activation='relu', input_shape=(1000,)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units=9, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model.compile(loss='categorical_crossentropy',optimizer='rmsprop',metrics=['acc',metrics.categorical_accuracy])\n",
        "    #################################################################################\n",
        "\n",
        "    #Keras Tokenizer\n",
        "    num_max = 1000\n",
        "    tok = Tokenizer(num_words=num_max)\n",
        "    tok.fit_on_texts(X)\n",
        "    X = tok.texts_to_matrix(X,mode='count')\n",
        "\n",
        "    #Label Encoder\n",
        "    encoder=preprocessing.LabelEncoder()\n",
        "    encoder.fit(y)\n",
        "    y=encoder.transform(y)\n",
        "    num_classes = np.max(y) + 1\n",
        "    y = utils.to_categorical(y,num_classes)\n",
        "\n",
        "    #Model Building\n",
        "    model.fit(X, y, epochs=10, batch_size=500,verbose=1,validation_split=0.2)\n",
        "\n",
        "    ##model_yaml = model.to_yaml()\n",
        "    ##model_yaml\n",
        "\n",
        "    model_json = model.to_json()\n",
        "    model_json\n",
        "\n",
        "    #saving The Models\n",
        "    with open('..\\\\models\\\\tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(tok,f)\n",
        "\n",
        "    ##with open(\"..\\\\models\\\\model.yaml\", \"w\") as yaml_file:\n",
        "    ##    yaml_file.write(model_yaml)\n",
        "\n",
        "    with open(\"..\\\\models\\\\model.json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "\n",
        "    model.save_weights(\"..\\\\models\\\\model.h5\")\n",
        "    print(\"Saved model to disk\")\n",
        "\n",
        "    with open('..\\\\models\\\\encoder.pkl', 'wb') as f:\n",
        "        pickle.dump(encoder,f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7BVp5lm-I6hY",
        "outputId": "7c3037da-a8fe-44dc-a4ec-63c25fe7f7c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                             content  \\\n",
            "0  I was contacted originally on XX XX XXXX via c...   \n",
            "1  Thanks for your response and update regarding ...   \n",
            "2  Dear Consumers Financial Protection Bureau   C...   \n",
            "3  Experian XXXX XXXX XXXX XXXX XXXX XXXX XXXX  X...   \n",
            "4  Doctors Business Bureau for            That de...   \n",
            "\n",
            "                      result  \n",
            "0   Action > Debt collection  \n",
            "1          Action > Mortgage  \n",
            "2      Action > Student loan  \n",
            "3  Action > Credit reporting  \n",
            "4   Action > Debt collection  \n",
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "<class 'pandas.core.series.Series'>\n",
            "Train Data Features: (6823, 2)\n",
            "Validation Data Features: (759, 2)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_3 (Dense)             (None, 1000)              1001000   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1000)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1024)              1025024   \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 9)                 9225      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2035249 (7.76 MB)\n",
            "Trainable params: 2035249 (7.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "11/11 [==============================] - 2s 154ms/step - loss: 2.1197 - acc: 0.4652 - categorical_accuracy: 0.4652 - val_loss: 1.2870 - val_acc: 0.6037 - val_categorical_accuracy: 0.6037\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 1.0352 - acc: 0.7059 - categorical_accuracy: 0.7059 - val_loss: 0.8255 - val_acc: 0.7751 - val_categorical_accuracy: 0.7751\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.7491 - acc: 0.7869 - categorical_accuracy: 0.7869 - val_loss: 0.8124 - val_acc: 0.7575 - val_categorical_accuracy: 0.7575\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 0.6221 - acc: 0.8170 - categorical_accuracy: 0.8170 - val_loss: 0.8304 - val_acc: 0.7670 - val_categorical_accuracy: 0.7670\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 2s 199ms/step - loss: 0.4604 - acc: 0.8653 - categorical_accuracy: 0.8653 - val_loss: 0.8256 - val_acc: 0.7626 - val_categorical_accuracy: 0.7626\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 2s 179ms/step - loss: 0.4001 - acc: 0.8728 - categorical_accuracy: 0.8728 - val_loss: 0.6605 - val_acc: 0.8125 - val_categorical_accuracy: 0.8125\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 1s 135ms/step - loss: 0.3389 - acc: 0.9020 - categorical_accuracy: 0.9020 - val_loss: 1.0708 - val_acc: 0.7436 - val_categorical_accuracy: 0.7436\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 1s 137ms/step - loss: 0.2486 - acc: 0.9300 - categorical_accuracy: 0.9300 - val_loss: 0.6276 - val_acc: 0.8220 - val_categorical_accuracy: 0.8220\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 2s 138ms/step - loss: 0.1838 - acc: 0.9483 - categorical_accuracy: 0.9483 - val_loss: 0.8285 - val_acc: 0.8059 - val_categorical_accuracy: 0.8059\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 2s 141ms/step - loss: 0.1625 - acc: 0.9514 - categorical_accuracy: 0.9514 - val_loss: 0.7098 - val_acc: 0.8161 - val_categorical_accuracy: 0.8161\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Method `model.to_yaml()` has been removed due to security risk of arbitrary code execution. Please use `model.to_json()` instead.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-da0b70116439>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mDLTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-8b68bc2a4489>\u001b[0m in \u001b[0;36mDLTrain\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodel_yaml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_yaml\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mmodel_yaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mto_yaml\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   3403\u001b[0m             \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mannounces\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0mposes\u001b[0m \u001b[0ma\u001b[0m \u001b[0msecurity\u001b[0m \u001b[0mrisk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3404\u001b[0m         \"\"\"\n\u001b[0;32m-> 3405\u001b[0;31m         raise RuntimeError(\n\u001b[0m\u001b[1;32m   3406\u001b[0m             \u001b[0;34m\"Method `model.to_yaml()` has been removed due to security risk of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3407\u001b[0m             \u001b[0;34m\"arbitrary code execution. Please use `model.to_json()` instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Method `model.to_yaml()` has been removed due to security risk of arbitrary code execution. Please use `model.to_json()` instead."
          ]
        }
      ],
      "source": [
        "DLTrain(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TmM_4DJI6hY"
      },
      "outputs": [],
      "source": [
        "def validation(text):\n",
        "\n",
        "    #Loading Tokenizer\n",
        "    with open('..\\\\models\\\\tokenizer.pkl', 'rb') as f:\n",
        "        tok = pickle.load(f)\n",
        "\n",
        "    #loading hyperparameter\n",
        "    with open(\"..\\\\models\\\\model.yaml\", \"r\") as yaml_file:\n",
        "        model_yaml = yaml_file.read()\n",
        "\n",
        "    #loading model\n",
        "    model = model_from_yaml(model_yaml)\n",
        "    model.load_weights(\"..\\\\models\\\\model.h5\")\n",
        "\n",
        "    #loading encoder for converting encoded labels to actual labels\n",
        "    with open('..\\\\models\\\\encoder.pkl', 'rb') as f:\n",
        "        encoder = pickle.load(f)\n",
        "\n",
        "    #inline text\n",
        "    testdataL = text\n",
        "\n",
        "\n",
        "\n",
        "    #tokenization of inline text\n",
        "    X_test=tok.texts_to_matrix(testdataL,mode='count')\n",
        "\n",
        "    #predicting for inline text\n",
        "    prediction = model.predict(np.array(X_test))\n",
        "\n",
        "    #exracting the labels for the predicted text\n",
        "    text_labels = encoder.classes_\n",
        "    predicted_label = text_labels[np.argmax(prediction)]\n",
        "    print(\"predicted category -->\",predicted_label)\n",
        "\n",
        "    prediction_prob = model.predict_proba(np.array([X_test[0]]))\n",
        "    confidence = prediction_prob[0][np.argmax(prediction_prob)]\n",
        "    print(\"confidence score -->\",str(round(confidence,2)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUeEed0sI6hY",
        "outputId": "22e8f061-3209-4566-89ba-c907ed1f0676"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "predicted category --> Action > Student loan\n",
            "confidence score --> 0.56\n"
          ]
        }
      ],
      "source": [
        "text = [\"I want a student loan\"]\n",
        "validation(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4823CBKCI6hZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}